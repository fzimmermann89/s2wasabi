### Semi-Supervised Learning forSpatially Regularized qMRI Reconstruction - Application to Simultaneous ùëá_1, ùêµ_0, and ùêµ_1  Mapping¬†
Source, Weights and Wasabi-MR Sequence (Pulseseq) for #1166, ISMRM 2023

Problem: Training a network to predict T1, B0 and B1 from WASABIT! magnitude images with only 10 slices of volunteer data
Approach: 
    - Pretraining with Synthetic Data
    - Finetuning with a combination of
            - Supervised Training on Synthetic Data
            - Teacher-Student Training with Seperate Augmentations and Random Recombinations of Parameter Maps
            - Self-Supervised (Masked) Training on In-vivo Data
Results: Noise-robust parameter map estimation with reduced cross-talk between parameter maps

Content:
    Sequence:
     - WASABI.seq: Pulsesq seq file
     - WASABI.py: PyPulsesq-Script for generating the sequence  
    Trained models
        - UNet-finetunded-724.model: Final UNet
        - UNet-pretrained-652.model: UNet after pretraining
        - MLP-684.model:  MLP-Baseline trained on synthetic data, as proposed by #2714 ISMRM 2022 
    Source Code:
        - pretrain.py: Simple skript for pretraining
        - fineune.py: ..and finetuning ;)


Unfortunalty, we cannot provide the in-vivo dataset used for fine-tuning.

-- This code will be cleaned up and seperated into the WASABI-specific part and a general example for the training regime for a future publication, 'till then, have fun and feel free to ask questions

(c) Felix Zimmermann, felix.zimmermann@ptb.de
All code is open source licensed under BSD-3 license.

